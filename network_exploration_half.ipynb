{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import utils\n",
    "import plot_utils\n",
    "from tqdm.notebook import tqdm, tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_public_dataset(tournament='World_Cup'):\n",
    "    \"\"\"\n",
    "    Load the json files with the matches, events, players and competitions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_folder : str, optional\n",
    "        the path to the folder where json files are stored.\n",
    "        \n",
    "    tournaments : list, optional\n",
    "        the list of tournaments to load. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        a tuple of four dictionaries, containing matches, events, players and competitions\n",
    "        \n",
    "    \"\"\"\n",
    "    # loading the matches and events data\n",
    "    matches, events = {}, {}\n",
    "    with open('events/events_%s.json' %tournament) as json_data:\n",
    "        events = json.load(json_data)\n",
    "    with open('matches/matches_%s.json' %tournament) as json_data:\n",
    "        matches = json.load(json_data)\n",
    "    \n",
    "    match_id2events = defaultdict(list)\n",
    "    match_id2match = defaultdict(dict)\n",
    "    for event in events:\n",
    "        match_id = event['matchId']\n",
    "        match_id2events[match_id].append(event)\n",
    "                                         \n",
    "    for match in matches:\n",
    "        match_id = match['wyId']\n",
    "        match_id2match[match_id] = match\n",
    "\n",
    "    # loading the players data\n",
    "    with open('players.json') as json_data:\n",
    "        players = json.load(json_data)\n",
    "    \n",
    "    player_id2player = defaultdict(dict)\n",
    "    for player in players:\n",
    "        player_id = player['wyId']\n",
    "        player_id2player[player_id] = player\n",
    "    \n",
    "    # loading the competitions data\n",
    "    teams={}\n",
    "    with open('teams.json') as json_data:\n",
    "        teams = json.load(json_data)\n",
    "    team_id2team = defaultdict(dict)\n",
    "    for team in teams:\n",
    "        team_id = team['wyId']\n",
    "        team_id2team[team_id] = team\n",
    "\n",
    "    #Corrigindo o nome do time Francês para igualar ao 'events'\n",
    "    team_id2team[3799]['name'] = 'Angers SCO'\n",
    "    \n",
    "    return match_id2match, match_id2events, player_id2player, team_id2team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passing_networks_half(match_id=2057954):\n",
    "    \"\"\"\n",
    "    Construct the passing networks of the teams in the match.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    match_id : int, optional\n",
    "        identifier of the match to plot\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        the two constructed networks, as networkx objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    # take the names of the two teams of the match\n",
    "    match_label = match_id2match[match_id]['label']\n",
    "    team1_name = match_label.split(' - ')[0].strip()\n",
    "    team2_name = match_label.split(' - ')[1].split(',')[0].strip()\n",
    "\n",
    "    # take all the events of the match\n",
    "    match_events = list(match_id2events[match_id])\n",
    "    match_events_df = pd.DataFrame(match_events)\n",
    "    first_half_max_duration = np.max(match_events_df[match_events_df['matchPeriod'] == '1H']['eventSec'])\n",
    "    # sum 1H time end to all the time in 2H\n",
    "    for event in match_events:\n",
    "        if event['matchPeriod'] == '2H':\n",
    "            event['eventSec'] += first_half_max_duration\n",
    "\n",
    "    team2pass2weight_1H = defaultdict(lambda: defaultdict(int))\n",
    "    team2pass2weight_2H = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for event, next_event, next_next_event in zip(match_events, match_events[1:], match_events[2:]):\n",
    "        try:\n",
    "            if event['matchPeriod'] == '1H':\n",
    "                if event['eventName'] == 'Pass' and 1801 in [tag['id'] for tag in event['tags']]:\n",
    "                    sender = player_id2player[event['playerId']]['shortName'].encode('ascii', 'strict').decode('unicode-escape')\n",
    "                    # if the next event of from a player of the same team\n",
    "                    if (next_event['teamId'] == event['teamId']) & (next_event['playerId'] != event['playerId']):\n",
    "                        receiver = player_id2player[next_event['playerId']]['shortName'].encode('ascii', 'strict').decode('unicode-escape')\n",
    "                        team2pass2weight_1H[team_id2team[event['teamId']]['name']][(sender, receiver)] += 1\n",
    "                    elif (next_next_event['teamId'] == event['teamId']) & (next_next_event['playerId'] != event['playerId']):\n",
    "                        receiver = player_id2player[next_next_event['playerId']]['shortName'].encode('ascii', 'strict').decode('unicode-escape')\n",
    "                        team2pass2weight_1H[team_id2team[event['teamId']]['name']][(sender, receiver)] += 1\n",
    "            else:\n",
    "                if event['eventName'] == 'Pass' and 1801 in [tag['id'] for tag in event['tags']]:\n",
    "                    sender = player_id2player[event['playerId']]['shortName'].encode('ascii', 'strict').decode('unicode-escape')\n",
    "                    # if the next event of from a player of the same team\n",
    "                    if (next_event['teamId'] == event['teamId']) & (next_event['playerId'] != event['playerId']):\n",
    "                        receiver = player_id2player[next_event['playerId']]['shortName'].encode('ascii', 'strict').decode('unicode-escape')\n",
    "                        team2pass2weight_2H[team_id2team[event['teamId']]['name']][(sender, receiver)] += 1\n",
    "                    elif (next_next_event['teamId'] == event['teamId']) & (next_next_event['playerId'] != event['playerId']):\n",
    "                        receiver = player_id2player[next_next_event['playerId']]['shortName'].encode('ascii', 'strict').decode('unicode-escape')\n",
    "                        team2pass2weight_2H[team_id2team[event['teamId']]['name']][(sender, receiver)] += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    # crete networkx graphs\n",
    "    G1_1H, G2_1H = nx.DiGraph(team=team1_name), nx.DiGraph(team=team2_name)\n",
    "    G1_2H, G2_2H = nx.DiGraph(team=team1_name), nx.DiGraph(team=team2_name)\n",
    "    for (sender, receiver), weight in team2pass2weight_1H[team1_name].items():\n",
    "        G1_1H.add_edge(sender, receiver, weight=weight)\n",
    "    for (sender, receiver), weight in team2pass2weight_1H[team2_name].items():\n",
    "        G2_1H.add_edge(sender, receiver, weight=weight)\n",
    "    for (sender, receiver), weight in team2pass2weight_2H[team1_name].items():\n",
    "        G1_2H.add_edge(sender, receiver, weight=weight)\n",
    "    for (sender, receiver), weight in team2pass2weight_2H[team2_name].items():\n",
    "        G2_2H.add_edge(sender, receiver, weight=weight)        \n",
    "    \n",
    "    return G1_1H, G2_1H, G1_2H, G2_2H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pith_pos_to_dict(data):\n",
    "\n",
    "    aux = data[['playerId', 'y', 'x']]\n",
    "    aux['nome'] = aux['playerId'].apply(lambda p: player_id2player[p]['shortName'].encode('ascii', 'strict').decode('unicode-escape'))\n",
    "\n",
    "    res = dict(zip(aux['nome'], zip(aux['x'], aux['y'])))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_passing_networks(G1, G2, df_pos_1H, df_pos_2H):\n",
    "    \"\"\"\n",
    "    Plot the two passing networks in input.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    G1 : networkx object\n",
    "        the object representing the first network\n",
    "        \n",
    "    G2 : networkx object\n",
    "        the object representing the second network\n",
    "    \"\"\"\n",
    "    #TO DO:\n",
    "    # ***** Plotar nodes e edges baseado no peso *****\n",
    "\n",
    "    player_pos1 = pith_pos_to_dict(df_pos_1H)\n",
    "    player_pos2 = pith_pos_to_dict(df_pos_2H)\n",
    "\n",
    "    fig, ax1 = plot_utils.pitch()\n",
    "    fig, ax2 = plot_utils.pitch()\n",
    "    \n",
    "\n",
    "    nome2degree = dict(G1.degree)\n",
    "    nx.draw(G1, pos=player_pos1 ,nodelist=list(nome2degree.keys()), \n",
    "            node_size=[np.power(deg , 2.6) for deg in nome2degree.values()], \n",
    "            node_color='lightgray', edge_color='black', edgecolors='black',\n",
    "            with_labels=True, font_weight='bold', alpha=0.9, verticalalignment='center', ax=ax1)\n",
    "    ax1.set_title('Real Madrid - 1º Tempo')\n",
    "    ax1.set_xlim([0, 100])\n",
    "    ax1.set_ylim([0, 100])\n",
    "    ax1.set_axis_on()\n",
    "\n",
    "    \n",
    "    nome2degree = dict(G2.degree)\n",
    "    nx.draw(G2, pos=player_pos2, nodelist=list(nome2degree.keys()), \n",
    "            node_size=[np.power(deg , 2.6) for deg in nome2degree.values()],\n",
    "            node_color='lightgray', edge_color='black', edgecolors='black',\n",
    "            with_labels=True, font_weight='bold', alpha=0.9, verticalalignment='center', ax=ax2)\n",
    "    ax2.set_title('Real Madrid - 2º Tempo')\n",
    "    ax2.set_xlim([0, 100])\n",
    "    ax2.set_ylim([0, 100])\n",
    "    ax2.set_axis_on()\n",
    "    \n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_net = next(iter(net_list))\n",
    "plot_passing_networks(net_list_1H[2565711][0], net_list_2H[2565711][0], pitch_coords_1H[pitch_coords_1H['matchId'] == 2565711], pitch_coords_2H[pitch_coords_2H['matchId'] == 2565711])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#degree_centrality\n",
    "def degree_centrality(netlist, i):\n",
    "    data_avg = []\n",
    "    data_std = []\n",
    "    data_min = []\n",
    "    data_max = []\n",
    "    for net in netlist.values():\n",
    "        data_avg.append(np.mean(list(nx.degree_centrality(net[i]).values())))\n",
    "        data_std.append(np.std(list(nx.degree_centrality(net[i]).values())))\n",
    "        data_min.append(min(list(nx.degree_centrality(net[i]).values())))\n",
    "        data_max.append(max(list(nx.degree_centrality(net[i]).values())))\n",
    "\n",
    "    return pd.DataFrame(list(zip(data_avg, data_std, data_min, data_max)), columns=[f'avg_degree_centrality_T{i+1}', f'std_degree_centrality_T{i+1}', f'min_degree_centrality_T{i+1}', f'max_degree_centrality_T{i+1}'])\n",
    "\n",
    "\n",
    "#betwenness_centrality\n",
    "def betwenness_centrality(netlist, i):\n",
    "    data_avg = []\n",
    "    data_std = []\n",
    "    data_min = []\n",
    "    data_max = []\n",
    "    for net in netlist.values():\n",
    "        data_avg.append(np.mean(list(nx.betweenness_centrality(net[i]).values())))\n",
    "        data_std.append(np.std(list(nx.betweenness_centrality(net[i]).values())))\n",
    "        data_min.append(min(list(nx.betweenness_centrality(net[i]).values())))\n",
    "        data_max.append(max(list(nx.betweenness_centrality(net[i]).values())))\n",
    "\n",
    "    return pd.DataFrame(list(zip(data_avg, data_std, data_min, data_max)), columns=[f'avg_betweenness_centrality_T{i+1}', f'std_betweenness_centrality_T{i+1}', f'min_betweenness_centrality_T{i+1}', f'max_betweenness_centrality_T{i+1}'])\n",
    "\n",
    "#closeness_centrality\n",
    "def closeness_centrality(netlist, i):\n",
    "    data_avg = []\n",
    "    data_std = []\n",
    "    data_min = []\n",
    "    data_max = [] \n",
    "    for net in netlist.values():\n",
    "        data_avg.append(np.mean(list(nx.closeness_centrality(net[i]).values())))\n",
    "        data_std.append(np.std(list(nx.closeness_centrality(net[i]).values())))\n",
    "        data_min.append(min(list(nx.closeness_centrality(net[i]).values())))\n",
    "        data_max.append(max(list(nx.closeness_centrality(net[i]).values())))\n",
    "\n",
    "    return pd.DataFrame(list(zip(data_avg, data_std, data_min, data_max)), columns=[f'avg_closeness_centrality_T{i+1}', f'std_closeness_centrality_T{i+1}', f'min_closeness_centrality_T{i+1}', f'max_closeness_centrality_T{i+1}'])\n",
    "\n",
    "#eigenvalue_centrality\n",
    "def eigenvector_centrality(netlist, i):\n",
    "    data_avg = []\n",
    "    data_std = []\n",
    "    data_min = []\n",
    "    data_max = []\n",
    "    for net in netlist.values():\n",
    "        data_avg.append(np.mean(list(nx.eigenvector_centrality(net[i]).values())))\n",
    "        data_std.append(np.std(list(nx.eigenvector_centrality(net[i]).values())))\n",
    "        data_min.append(min(list(nx.eigenvector_centrality(net[i]).values())))\n",
    "        data_max.append(max(list(nx.eigenvector_centrality(net[i]).values())))\n",
    "\n",
    "    return pd.DataFrame(list(zip(data_avg, data_std, data_min, data_max)), columns=[f'avg_eigenvector_centrality_T{i+1}', f'std_eigenvector_centrality_T{i+1}', f'min_eigenvector_centrality_T{i+1}', f'max_eigenvector_centrality_T{i+1}'])\n",
    "\n",
    "#clustering coefficient\n",
    "def clustering_coefficient(netlist, i):\n",
    "    data_avg = []\n",
    "    data_std = []\n",
    "    data_min = []\n",
    "    data_max = []\n",
    "    for net in netlist.values():\n",
    "        data_avg.append(np.mean(list(nx.clustering(net[i]).values())))\n",
    "        data_std.append(np.std(list(nx.clustering(net[i]).values())))\n",
    "        data_min.append(min(list(nx.clustering(net[i]).values())))\n",
    "        data_max.append(max(list(nx.clustering(net[i]).values())))\n",
    "\n",
    "    return pd.DataFrame(list(zip(data_avg, data_std, data_min, data_max)), columns=[f'avg_clustering_T{i+1}', f'std_clustering_T{i+1}', f'min_clustering_T{i+1}', f'max_clustering_T{i+1}'])\n",
    "\n",
    "#average shortest path lenght\n",
    "def avg_shortest_path(netlist, i):\n",
    "    data_avg = []\n",
    "    for net in netlist.values():\n",
    "        data_avg.append((nx.average_shortest_path_length(net[i])))\n",
    "\n",
    "    return pd.DataFrame(data_avg, columns=[f'avg_shortest_path_T{i+1}'])\n",
    "\n",
    "#position centroid\n",
    "def centroid(netlist, pitch_coords, i):\n",
    "    centroid = pitch_coords.groupby(['matchId', 'teamId']).agg({'y':([np.mean, np.std]), 'x':([np.mean, np.std])}).reset_index()\n",
    "    centroid.columns = ['matchId', 'teamId', f'mean_centroid_y_T{i+1}', f'std_centroid_y_T{i+1}', f'mean_centroid_x_T{i+1}', f'std_centroid_x_T{i+1}']\n",
    "    \n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOURNAMENT: Spain\n",
      "carregando dados\n",
      "convertendo tags\n",
      "gerando coordenadas médias\n"
     ]
    }
   ],
   "source": [
    "for tournament in ['Spain']:\n",
    "    print(\"TOURNAMENT: %s\" %tournament)\n",
    "\n",
    "    # carrega dados\n",
    "    print(\"carregando dados\")\n",
    "    match_id2match, match_id2events, player_id2player, team_id2team = load_public_dataset(tournament)\n",
    "    matches = pd.read_json('matches\\matches_%s.json' %tournament)\n",
    "    events = pd.read_json('events\\events_%s.json' %tournament)\n",
    "\n",
    "    #consertando erros do dataset\n",
    "    if tournament == 'France':\n",
    "        matches.loc[1, 'winner'] = 3804\n",
    "        matches.loc[37, 'winner'] = 3770\n",
    "    elif tournament == 'Germany':\n",
    "        matches.loc[48, 'winner'] = 2450\n",
    "        matches.loc[102, 'winner'] = 2449\n",
    "\n",
    "    #tira penalidades\n",
    "    events = events[events['matchPeriod'] != 'P']\n",
    "\n",
    "    # converte coluna 'tags' par algo manipulável\n",
    "    print(\"convertendo tags\")\n",
    "    events['tags'] = events['tags'].map(utils.converting_tags)\n",
    "\n",
    "    # gera coordenada média de cada node\n",
    "    print(\"gerando coordenadas médias\")\n",
    "    # gera coordenada média de cada node - 1H\n",
    "    passes_1H = events[(events['eventName'] == 'Pass') & (events['matchPeriod'] == '1H') & (events['tags'].apply(lambda x: (1801 in x)))]\n",
    "\n",
    "    coords_1H = passes_1H['positions'].apply(pd.Series)[0].apply(pd.Series)\n",
    "    passes_1H = passes_1H.join(coords_1H)\n",
    "    passes_1H = passes_1H.drop(['positions'], axis = 1)\n",
    "\n",
    "    pitch_coords_1H = passes_1H.groupby(['matchId', 'teamId', 'playerId']).agg(y=('y','mean'), x=('x','mean'))\n",
    "    pitch_coords_1H = pitch_coords_1H.reset_index()\n",
    "\n",
    "    # gera coordenada média de cada node - 2H\n",
    "    passes_2H = events[(events['eventName'] == 'Pass') & (events['matchPeriod'] != '1H') & (events['tags'].apply(lambda x: (1801 in x)))]\n",
    "\n",
    "    coords_2H = passes_2H['positions'].apply(pd.Series)[0].apply(pd.Series)\n",
    "    passes_2H = passes_2H.join(coords_2H)\n",
    "    passes_2H = passes_2H.drop(['positions'], axis = 1)\n",
    "\n",
    "    pitch_coords_2H = passes_2H.groupby(['matchId', 'teamId', 'playerId']).agg(y=('y','mean'), x=('x','mean'))\n",
    "    pitch_coords_2H = pitch_coords_2H.reset_index()\n",
    "\n",
    "    #gera redes\n",
    "    net_list_1H = defaultdict(tuple)\n",
    "    net_list_2H = defaultdict(tuple)\n",
    "    for matchID in match_id2match.keys():    \n",
    "        G1, G2, G3, G4 = passing_networks_half(match_id = matchID)\n",
    "        par_1H = (G1, G2)\n",
    "        par_2H = (G3, G4)\n",
    "        net_list_1H[matchID] = par_1H\n",
    "        net_list_2H[matchID] = par_2H\n",
    "\n",
    "\n",
    "    # - Gera bases -\n",
    "    print(\"gerando base\")\n",
    "\n",
    "    # -- HOME --\n",
    "\n",
    "    # 1 HALF\n",
    "\n",
    "    dfh_1h = matches[['wyId']]\n",
    "    dfh_1h = dfh_1h.rename(columns={'wyId':'matchID'})\n",
    "\n",
    "    home_l = []\n",
    "    away_l = []\n",
    "\n",
    "    for match in matches['teamsData']:\n",
    "        t0 = list(match.values())[0]\n",
    "        t1 = list(match.values())[1]\n",
    "\n",
    "        if t0['side'] == 'home':\n",
    "            home_l.append(t0['teamId'])\n",
    "            away_l.append(t1['teamId'])  \n",
    "        else: \n",
    "            away_l.append(t0['teamId'])\n",
    "            home_l.append(t1['teamId'])\n",
    "\n",
    "    dfh_1h['team1_ID'] = home_l\n",
    "    dfh_1h['team2_ID'] = away_l\n",
    "\n",
    "    dfh_1h['date'] = matches['date']\n",
    "\n",
    "    dfh_1h['tournament'] = tournament\n",
    "\n",
    "    closeness = closeness_centrality(net_list_1H, 0)\n",
    "    degree = degree_centrality(net_list_1H, 0)\n",
    "    betwenness = betwenness_centrality(net_list_1H, 0)\n",
    "    eigenvector = eigenvector_centrality(net_list_1H, 0)\n",
    "    clustering = clustering_coefficient(net_list_1H, 0)\n",
    "    shortest_path = avg_shortest_path(net_list_1H, 0)\n",
    "    df_centroid = centroid(net_list_1H, pitch_coords_1H, 0)\n",
    "\n",
    "    dfh_1h = dfh_1h.join(closeness)\n",
    "    dfh_1h = dfh_1h.join(degree)\n",
    "    dfh_1h = dfh_1h.join(betwenness)\n",
    "    dfh_1h = dfh_1h.join(eigenvector)\n",
    "    dfh_1h = dfh_1h.join(clustering)\n",
    "    dfh_1h = dfh_1h.join(shortest_path)\n",
    "\n",
    "    dfh_1h = dfh_1h.merge(df_centroid, how='left', left_on=['matchID', 'team1_ID'], right_on=['matchId', 'teamId']).drop(['matchId', 'teamId'], axis=1)\n",
    "\n",
    "    # 2 HALF\n",
    "\n",
    "    dfh_2h = matches[['wyId']]\n",
    "    dfh_2h = dfh_2h.rename(columns={'wyId':'matchID'})\n",
    "\n",
    "    home_l = []\n",
    "    away_l = []\n",
    "\n",
    "    for match in matches['teamsData']:\n",
    "        t0 = list(match.values())[0]\n",
    "        t1 = list(match.values())[1]\n",
    "\n",
    "        if t0['side'] == 'home':\n",
    "            home_l.append(t0['teamId'])\n",
    "            away_l.append(t1['teamId'])  \n",
    "        else: \n",
    "            away_l.append(t0['teamId'])\n",
    "            home_l.append(t1['teamId'])\n",
    "\n",
    "    dfh_2h['team1_ID'] = home_l\n",
    "    dfh_2h['team2_ID'] = away_l\n",
    "\n",
    "    dfh_2h['date'] = matches['date']\n",
    "\n",
    "    dfh_2h['tournament'] = tournament\n",
    "\n",
    "    closeness = closeness_centrality(net_list_2H, 0)\n",
    "    degree = degree_centrality(net_list_2H, 0)\n",
    "    betwenness = betwenness_centrality(net_list_2H, 0)\n",
    "    eigenvector = eigenvector_centrality(net_list_2H, 0)\n",
    "    clustering = clustering_coefficient(net_list_2H, 0)\n",
    "    shortest_path = avg_shortest_path(net_list_2H, 0)\n",
    "    df_centroid = centroid(net_list_2H, pitch_coords_2H, 0)\n",
    "\n",
    "    dfh_2h = dfh_2h.join(closeness)\n",
    "    dfh_2h = dfh_2h.join(degree)\n",
    "    dfh_2h = dfh_2h.join(betwenness)\n",
    "    dfh_2h = dfh_2h.join(eigenvector)\n",
    "    dfh_2h = dfh_2h.join(clustering)\n",
    "    dfh_2h = dfh_2h.join(shortest_path)\n",
    "\n",
    "    dfh_2h = dfh_2h.merge(df_centroid, how='left', left_on=['matchID', 'team1_ID'], right_on=['matchId', 'teamId']).drop(['matchId', 'teamId'], axis=1)\n",
    "\n",
    "    # -- AWAY --\n",
    "\n",
    "    # 1 HALF\n",
    "\n",
    "    dfa_1h = matches[['wyId']]\n",
    "    dfa_1h = dfa_1h.rename(columns={'wyId':'matchID'})\n",
    "\n",
    "    dfa_1h['team1_ID'] = home_l\n",
    "    dfa_1h['team2_ID'] = away_l\n",
    "\n",
    "    dfa_1h['date'] = matches['date']\n",
    "\n",
    "    dfa_1h['tournament'] = tournament\n",
    "\n",
    "    closeness = closeness_centrality(net_list_1H, 1)\n",
    "    degree = degree_centrality(net_list_1H, 1)\n",
    "    betwenness = betwenness_centrality(net_list_1H, 1)\n",
    "    eigenvector = eigenvector_centrality(net_list_1H, 1)\n",
    "    clustering = clustering_coefficient(net_list_1H, 1)\n",
    "    shortest_path = avg_shortest_path(net_list_1H, 1)\n",
    "    df_centroid = centroid(net_list_1H, pitch_coords_1H, 1)\n",
    "\n",
    "    dfa_1h = dfa_1h.join(closeness)\n",
    "    dfa_1h = dfa_1h.join(degree)\n",
    "    dfa_1h = dfa_1h.join(betwenness)\n",
    "    dfa_1h = dfa_1h.join(eigenvector)\n",
    "    dfa_1h = dfa_1h.join(clustering)\n",
    "    dfa_1h = dfa_1h.join(shortest_path)\n",
    "\n",
    "    dfa_1h = dfa_1h.merge(df_centroid, how='left', left_on=['matchID', 'team2_ID'], right_on=['matchId', 'teamId']).drop(['matchId', 'teamId'], axis=1)\n",
    "\n",
    "    # 2 HALF\n",
    "\n",
    "    dfa_2h = matches[['wyId']]\n",
    "    dfa_2h = dfa_2h.rename(columns={'wyId':'matchID'})\n",
    "\n",
    "    dfa_2h['team1_ID'] = home_l\n",
    "    dfa_2h['team2_ID'] = away_l\n",
    "\n",
    "    dfa_2h['date'] = matches['date']\n",
    "\n",
    "    dfa_2h['tournament'] = tournament\n",
    "\n",
    "    closeness = closeness_centrality(net_list_2H, 1)\n",
    "    degree = degree_centrality(net_list_2H, 1)\n",
    "    betwenness = betwenness_centrality(net_list_2H, 1)\n",
    "    eigenvector = eigenvector_centrality(net_list_2H, 1)\n",
    "    clustering = clustering_coefficient(net_list_2H, 1)\n",
    "    shortest_path = avg_shortest_path(net_list_2H, 1)\n",
    "    df_centroid = centroid(net_list_2H, pitch_coords_2H, 1)\n",
    "\n",
    "    dfa_2h = dfa_2h.join(closeness)\n",
    "    dfa_2h = dfa_2h.join(degree)\n",
    "    dfa_2h = dfa_2h.join(betwenness)\n",
    "    dfa_2h = dfa_2h.join(eigenvector)\n",
    "    dfa_2h = dfa_2h.join(clustering)\n",
    "    dfa_2h = dfa_2h.join(shortest_path)\n",
    "\n",
    "    dfa_2h = dfa_2h.merge(df_centroid, how='left', left_on=['matchID', 'team2_ID'], right_on=['matchId', 'teamId']).drop(['matchId', 'teamId'], axis=1)\n",
    "\n",
    "\n",
    "    #definindo vencedor e retirando empates apenas do dfh_1h (df home)\n",
    "    dfh_1h['winner'] = matches['winner']\n",
    "    dfh_1h = dfh_1h.loc[dfh_1h['winner'] != 0]\n",
    "    dfh_1h['winner'] = np.where((dfh_1h['winner'] == dfh_1h['team1_ID']), 1, 0)\n",
    "    #definindo vencedor e retirando empates apenas do dfh_1h (df home)\n",
    "    dfh_2h['winner'] = matches['winner']\n",
    "    dfh_2h = dfh_2h.loc[dfh_2h['winner'] != 0]\n",
    "    dfh_2h['winner'] = np.where((dfh_2h['winner'] == dfh_2h['team1_ID']), 1, 0)\n",
    "\n",
    "    dfa_1h = dfa_1h.loc[matches['winner'] != 0]\n",
    "    dfa_2h = dfa_2h.loc[matches['winner'] != 0]\n",
    "\n",
    "\n",
    "    print(\"gerando csv\")\n",
    "    dfh_1h.to_csv('analitics/network_%s_home_1h.csv' %tournament, encoding='utf-8', index=False)\n",
    "    dfa_1h.to_csv('analitics/network_%s_away_1h.csv' %tournament, encoding='utf-8', index=False)\n",
    "    dfh_2h.to_csv('analitics/network_%s_home_2h.csv' %tournament, encoding='utf-8', index=False)\n",
    "    dfa_2h.to_csv('analitics/network_%s_away_2h.csv' %tournament, encoding='utf-8', index=False)\n",
    "\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matchID</th>\n",
       "      <th>team1_ID</th>\n",
       "      <th>team2_ID</th>\n",
       "      <th>date</th>\n",
       "      <th>tournament</th>\n",
       "      <th>avg_closeness_centrality_T1</th>\n",
       "      <th>std_closeness_centrality_T1</th>\n",
       "      <th>min_closeness_centrality_T1</th>\n",
       "      <th>max_closeness_centrality_T1</th>\n",
       "      <th>avg_degree_centrality_T1</th>\n",
       "      <th>std_degree_centrality_T1</th>\n",
       "      <th>min_degree_centrality_T1</th>\n",
       "      <th>max_degree_centrality_T1</th>\n",
       "      <th>avg_betweenness_centrality_T1</th>\n",
       "      <th>std_betweenness_centrality_T1</th>\n",
       "      <th>min_betweenness_centrality_T1</th>\n",
       "      <th>max_betweenness_centrality_T1</th>\n",
       "      <th>avg_eigenvector_centrality_T1</th>\n",
       "      <th>std_eigenvector_centrality_T1</th>\n",
       "      <th>min_eigenvector_centrality_T1</th>\n",
       "      <th>max_eigenvector_centrality_T1</th>\n",
       "      <th>avg_clustering_T1</th>\n",
       "      <th>std_clustering_T1</th>\n",
       "      <th>min_clustering_T1</th>\n",
       "      <th>max_clustering_T1</th>\n",
       "      <th>avg_shortest_path_T1</th>\n",
       "      <th>mean_centroid_y_T1</th>\n",
       "      <th>std_centroid_y_T1</th>\n",
       "      <th>mean_centroid_x_T1</th>\n",
       "      <th>std_centroid_x_T1</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>2565711</td>\n",
       "      <td>675</td>\n",
       "      <td>676</td>\n",
       "      <td>December 23, 2017 at 1:00:00 PM GMT+1</td>\n",
       "      <td>Spain</td>\n",
       "      <td>0.789211</td>\n",
       "      <td>0.087841</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.436364</td>\n",
       "      <td>0.317011</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.031313</td>\n",
       "      <td>0.026771</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.104286</td>\n",
       "      <td>0.2982</td>\n",
       "      <td>0.044563</td>\n",
       "      <td>0.224298</td>\n",
       "      <td>0.389932</td>\n",
       "      <td>0.793498</td>\n",
       "      <td>0.081348</td>\n",
       "      <td>0.655556</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.281818</td>\n",
       "      <td>45.028408</td>\n",
       "      <td>19.077484</td>\n",
       "      <td>45.229677</td>\n",
       "      <td>15.616948</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     matchID  team1_ID  team2_ID                                   date  \\\n",
       "213  2565711       675       676  December 23, 2017 at 1:00:00 PM GMT+1   \n",
       "\n",
       "    tournament  avg_closeness_centrality_T1  std_closeness_centrality_T1  \\\n",
       "213      Spain                     0.789211                     0.087841   \n",
       "\n",
       "     min_closeness_centrality_T1  max_closeness_centrality_T1  \\\n",
       "213                     0.666667                          1.0   \n",
       "\n",
       "     avg_degree_centrality_T1  std_degree_centrality_T1  \\\n",
       "213                  1.436364                  0.317011   \n",
       "\n",
       "     min_degree_centrality_T1  max_degree_centrality_T1  \\\n",
       "213                       0.8                       2.0   \n",
       "\n",
       "     avg_betweenness_centrality_T1  std_betweenness_centrality_T1  \\\n",
       "213                       0.031313                       0.026771   \n",
       "\n",
       "     min_betweenness_centrality_T1  max_betweenness_centrality_T1  \\\n",
       "213                            0.0                       0.104286   \n",
       "\n",
       "     avg_eigenvector_centrality_T1  std_eigenvector_centrality_T1  \\\n",
       "213                         0.2982                       0.044563   \n",
       "\n",
       "     min_eigenvector_centrality_T1  max_eigenvector_centrality_T1  \\\n",
       "213                       0.224298                       0.389932   \n",
       "\n",
       "     avg_clustering_T1  std_clustering_T1  min_clustering_T1  \\\n",
       "213           0.793498           0.081348           0.655556   \n",
       "\n",
       "     max_clustering_T1  avg_shortest_path_T1  mean_centroid_y_T1  \\\n",
       "213               0.96              1.281818           45.028408   \n",
       "\n",
       "     std_centroid_y_T1  mean_centroid_x_T1  std_centroid_x_T1  winner  \n",
       "213          19.077484           45.229677          15.616948       0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfh_1h[(dfa_1h['team1_ID'] == 675) & (dfa_1h['team2_ID'] == 676)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L. Suárez': 12,\n",
       " 'Sergio Busquets': 19,\n",
       " 'Piqué': 13,\n",
       " 'T. Vermaelen': 12,\n",
       " 'Jordi Alba': 14,\n",
       " 'I. Rakitić': 13,\n",
       " 'M. ter Stegen': 9,\n",
       " 'Paulinho': 16,\n",
       " 'Sergi Roberto': 14,\n",
       " 'Iniesta': 13,\n",
       " 'L. Messi': 15}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(net_list_1H[2565711][1].degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([15, 20, 15, 8, 14, 13, 18, 16, 15, 14, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16641c8cfe17a9d73f3b6f813f8e13124430606dcf56459ef4dd2dad73abfa35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
